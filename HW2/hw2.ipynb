{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.backends import cudnn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import alexnet\n",
    "from torchvision.models import resnet34, vgg16\n",
    "from caltech_dataset import Caltech\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "\n",
    "NUM_CLASSES = 101\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-5\n",
    "NUM_EPOCHS = 30\n",
    "STEP_SIZE = 20\n",
    "GAMMA = 0.1\n",
    "LOG_FREQUENCY = 10\n",
    "DATA_DIR = '101_ObjectCategories/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                      ])\n",
    "\n",
    "eval_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 2892\n",
      "Valid Dataset: 2892\n",
      "Test Dataset: 2893\n"
     ]
    }
   ],
   "source": [
    "train_val_dataset = Caltech(DATA_DIR, split='train', transform=train_transform)\n",
    "\n",
    "test_dataset = Caltech(DATA_DIR, split='test', transform=eval_transform)\n",
    "\n",
    "train_indexes, val_indexes = train_test_split(train_val_dataset.indexes, train_size=0.5, random_state=42,\n",
    "                                              stratify=train_val_dataset.labels)\n",
    "\n",
    "# split the indices for the train / val split\n",
    "train_dataset = Subset(train_val_dataset, train_indexes)\n",
    "val_dataset = Subset(train_val_dataset, val_indexes)\n",
    "\n",
    "# Check dataset sizes\n",
    "print('Train Dataset: {}'.format(len(train_dataset)))\n",
    "print('Valid Dataset: {}'.format(len(val_dataset)))\n",
    "print('Test Dataset: {}'.format(len(test_dataset)))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = alexnet() \n",
    "net.classifier[6] = nn.Linear(4096, NUM_CLASSES) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "parameters_to_optimize = net.parameters()\n",
    "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(DEVICE)\n",
    "\n",
    "cudnn.benchmark\n",
    "current_step = 0\n",
    "min_accuracy = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Starting epoch {}/{}, LR = {}'.format(epoch + 1, NUM_EPOCHS, scheduler.get_lr()))\n",
    "\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        net.train()  \n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs = net(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        if current_step % LOG_FREQUENCY == 0:\n",
    "            print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        current_step += 1\n",
    "\n",
    "    net.train(False)\n",
    "    val_loss_epoch = 0\n",
    "    val_iter = 0\n",
    "    val_samples = 0\n",
    "    numCorr = 0\n",
    "    for inputs, targets in val_dataloader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        val_iter += 1\n",
    "        val_samples += inputs.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        val_loss = criterion(outputs, targets)\n",
    "        val_loss_epoch += val_loss.data.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        numCorr += torch.sum(predicted == targets.data).data.item()\n",
    "    val_accuracy = (numCorr / val_samples)\n",
    "    avg_val_loss = val_loss_epoch / val_iter\n",
    "    print('Validation: Epoch = {} | Loss = {} | Accuracy = {}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
    "    if val_accuracy > min_accuracy:\n",
    "        min_accuracy = val_accuracy\n",
    "        best_model = net\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_model.to(DEVICE)\n",
    "best_model.train(False)\n",
    "\n",
    "running_corrects = 0\n",
    "for images, labels in tqdm(val_dataloader):\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    # Forward Pass\n",
    "    outputs = best_model(images)\n",
    "\n",
    "    # Get predictions\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Update Corrects\n",
    "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = running_corrects / float(len(test_dataset))\n",
    "\n",
    "print('Validation Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_model.to(DEVICE) \n",
    "best_model.train(False) \n",
    "\n",
    "running_corrects = 0\n",
    "for images, labels in tqdm(test_dataloader):\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    # Forward Pass\n",
    "    outputs = best_model(images)\n",
    "\n",
    "    # Get predictions\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Update Corrects\n",
    "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = running_corrects / float(len(test_dataset))\n",
    "\n",
    "print('Test Accuracy: {}'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading pre trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = alexnet(pretrained=True)\n",
    "net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "parameters_to_optimize = net.parameters()\n",
    "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the mean and std to the ImageNet ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                                      ])\n",
    "\n",
    "eval_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run experiments with hyperparameters with this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LR = 0.01\n",
    "NUM_EPOCHS = 30\n",
    "STEP_SIZE = 20\n",
    "GAMMA = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [0, 3, 6, 8, 10]\n",
    "for layer in conv_layers:\n",
    "    net.features[layer].weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layers = [1, 4, 6]\n",
    "for layer in fc_layers:\n",
    "    net.classifier[layer].weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfreeze all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [0, 3, 6, 8, 10]\n",
    "fc_layers = [1, 4, 6]\n",
    "for layer in conv_layers:\n",
    "    net.features[layer].weight.requires_grad = True\n",
    "for layer in fc_layers:\n",
    "    net.classifier[layer].weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run experiments on transformations with this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.RandomRotation((-20, +20)),\n",
    "                                      transforms.ColorJitter(),\n",
    "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Beyond Alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet34 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet34(pretrained=True)\n",
    "net.fc = nn.Linear(512, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG16 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = vgg16(pretrained=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

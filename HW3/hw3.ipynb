{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Deep Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from gradient_reversal import mygradientreversalnet\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "\n",
    "NUM_CLASSES = 7\n",
    "NUM_DOMAINS = 2\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.005 \n",
    "MOMENTUM = 0.9  \n",
    "WEIGHT_DECAY = 5e-5  \n",
    "NUM_EPOCHS = 20 \n",
    "STEP_SIZE = 10  \n",
    "GAMMA = 0.1  \n",
    "LOG_FREQUENCY = 10\n",
    "\n",
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize(256),  \n",
    "                                      transforms.CenterCrop(224), \n",
    "                                      transforms.ToTensor(),  \n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                      ])\n",
    "\n",
    "eval_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"PACS\"\n",
    "\n",
    "photos = torchvision.datasets.ImageFolder(os.path.join(root, \"photo\"), transform=train_transform)\n",
    "art_paintings = torchvision.datasets.ImageFolder(os.path.join(root, \"art_painting\"), transform=train_transform)\n",
    "\n",
    "train_dataloader = DataLoader(photos, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, drop_last=True)\n",
    "test_dataloader = DataLoader(art_paintings, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mygradientreversalnet(pretrained=True)\n",
    "\n",
    "\n",
    "# Copy the classifier weights into the dann_classifier:\n",
    "for i in range(len(net.classifier)):\n",
    "    if (type(net.classifier[i])) not in [nn.ReLU, nn.Dropout]:\n",
    "        net.dann_classifier[i].weight.data = net.classifier[i].weight.data\n",
    "        net.dann_classifier[i].bias.data = net.classifier[i].bias.data\n",
    "\n",
    "# Change the last layers of the two branches in order to have the correct number of classes:\n",
    "net.classifier[6] = nn.Linear(4096, NUM_CLASSES)\n",
    "net.dann_classifier[6] = nn.Linear(4096, NUM_DOMAINS)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "parameters_to_optimize = net.parameters()\n",
    "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/20, LR = [0.005]\n",
      "Step 0, Loss 1.8622608184814453\n",
      "Step 10, Loss 0.5696645975112915\n",
      "Step 20, Loss 0.2044483721256256\n",
      "Step 30, Loss 0.3768826127052307\n",
      "Step 40, Loss 0.29052311182022095\n",
      "Step 50, Loss 0.14955906569957733\n",
      "Starting epoch 2/20, LR = [0.005]\n",
      "Step 60, Loss 0.5817769169807434\n",
      "Step 70, Loss 0.15583392977714539\n",
      "Step 80, Loss 0.2816635072231293\n",
      "Step 90, Loss 0.26235324144363403\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-d4ca83ece892>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m   \u001B[0;32mfor\u001B[0m \u001B[0mimages\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtrain_dataloader\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;31m# Bring data over the device of choice\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m     \u001B[0mimages\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimages\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m     \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "net = net.to(DEVICE)\n",
    "cudnn.benchmark\n",
    "current_step = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
    "\n",
    "  # Iterate over the dataset\n",
    "  for images, labels in train_dataloader:\n",
    "    # Bring data over the device of choice\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    net.train() \n",
    "\n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    outputs = net(images)\n",
    "    # Compute loss based onoutput and ground truth\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Log loss\n",
    "    if current_step % LOG_FREQUENCY == 0:\n",
    "      print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "\n",
    "    # Compute gradients for each layer and update weights\n",
    "    loss.backward()  # backward pass: computes gradients\n",
    "    optimizer.step() # update weights based on accumulated gradients\n",
    "\n",
    "    current_step += 1\n",
    "\n",
    "  # Step the scheduler\n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(DEVICE)\n",
    "cudnn.benchmark\n",
    "current_step = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Starting epoch {}/{}, LR = {}'.format(epoch + 1, NUM_EPOCHS, scheduler.get_lr()))\n",
    "\n",
    "    # Iterate over the dataset\n",
    "    for images, labels in train_dataloader:\n",
    "        # Bring data over the device of choice\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        net.train()\n",
    "        \n",
    "        optimizer.zero_grad()  \n",
    "\n",
    "        # 1):\n",
    "        outputs = net(images)  # Forward pass to the network\n",
    "        loss = criterion(outputs, labels)  # Compute loss based on output and ground truth\n",
    "        if current_step % LOG_FREQUENCY == 0:  # Log loss\n",
    "            print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "        # Compute gradients for each layer and update weights\n",
    "        loss.backward()  # backward pass: computes gradients\n",
    "\n",
    "        #  2)\n",
    "        domain_labels = torch.zeros(labels.size(), dtype=torch.int64)\n",
    "        domain_labels = domain_labels.to(DEVICE)\n",
    "        outputs = net(images, alpha=ALPHA)  # Forward pass to the network\n",
    "        loss2 = criterion(outputs, domain_labels)  # Compute loss based on output and ground truth\n",
    "        if current_step % LOG_FREQUENCY == 0:  # Log loss\n",
    "            print('Step {}, Loss {}'.format(current_step, loss2.item()))\n",
    "        # Compute gradients for each layer and update weights\n",
    "        loss2.backward()  # backward pass: computes gradients\n",
    "\n",
    "        #  3)\n",
    "        domain_labels = torch.ones(labels.size(), dtype=torch.int64)\n",
    "        domain_labels = domain_labels.to(DEVICE)\n",
    "        target_images = next(iter(test_dataloader))[0].to(DEVICE)\n",
    "        outputs = net(target_images, alpha=ALPHA)  # Forward pass to the network\n",
    "        loss3 = criterion(outputs, domain_labels)  # Compute loss based on output and ground truth\n",
    "        if current_step % LOG_FREQUENCY == 0:  # Log loss\n",
    "            print('Step {}, Loss {}'.format(current_step, loss3.item()))\n",
    "        # Compute gradients for each layer and update weights\n",
    "        loss3.backward()  # backward pass: computes gradients\n",
    "\n",
    "\n",
    "        optimizer.step()  # update weights based on accumulated gradients\n",
    "\n",
    "        current_step += 1\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(DEVICE)  # this will bring the network to GPU if DEVICE is cuda\n",
    "net.train(False)  # Set Network to evaluation mode\n",
    "\n",
    "running_corrects = 0\n",
    "classes_list = []\n",
    "predictions_list = []\n",
    "\n",
    "for images, labels in tqdm(test_dataloader):\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "  \n",
    "    # Forward Pass\n",
    "    outputs = net(images)\n",
    "\n",
    "    # Get predictions\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    \n",
    "    classes_list = classes_list + labels.data.tolist()\n",
    "    predictions_list = predictions_list + preds.data.tolist()\n",
    "    \n",
    "    # Update Corrects\n",
    "    running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = running_corrects / float(len(art_paintings))\n",
    "\n",
    "print('Test Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(classes_list, predictions_list)\n",
    "f, ax = plt.subplots(figsize=(7, 7))\n",
    "ax = sns.heatmap(confusion, annot=True, square=True, cbar=False, fmt=\"d\", annot_kws={\"size\": 14}, cmap='GnBu')\n",
    "ax.set_xticklabels(photos.classes)\n",
    "ax.set_yticklabels(photos.classes, rotation=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}